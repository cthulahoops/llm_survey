<!DOCTYPE html>
<html>
    <head>
        <title>LLM Survey</title>
        {% include "head.html.j2" %}
    </head>
    <body>

        <h1>LLM Survey</h1>

        {% include "github-banner.html.j2" %}
        <main class=contained>

<p>One challenge is to choose an evaluator to mark the LLM outputs. The evaluator needs to fairly
consistent over time, produce good results and not cost too much. I've chosen {{ prompt.evaluation_model }}
but here's some scores with different models.</p>

            <table class="highlight-hover">
                <thead>
                <tr>
                    <th class="column-header">Model</th>
                    {% for ev in evaluation_models %}
                    <th class="column-header"><div>{{ ev | model_name }}</div></th>
                    {% endfor %}
                </tr>
                </thead>
                {% for model in models %}
                    <tr>
                        <td>
                            {{ model | model_link }}
                        </td>

                        {% for ev in evaluation_models %}
                        {% with scores = outputs.model_scores(model, ev) %}
                        {% if scores %}
                        <td title="N={{ scores|length }}" style="--data-value: {{ (scores | average) / 10 }}">
                            {{ "%.1f"|format( scores | average )}}
                        </td>
                        {% else %}
                        <td></td>
                        {% endif %}
                        {% endwith %}
                        {% endfor %}
                    </tr>
                {% endfor %}
            </table>
        </main>
    </body>
</html>
